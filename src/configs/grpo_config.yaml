# The hyperparameters for the GRPO training are defined to be the same as the ones in the original paper:
# https://arxiv.org/pdf/2402.03300
bf16: true
use_liger_kernel: true

# sampling
num_generations: 8
max_completion_length: 4096

# training
do_train: true
output_dir: "/scratch/prj/charnu/ft_weights/mental-healer/grpo_qwen8/"
per_device_train_batch_size: 4
gradient_accumulation_steps: 8
num_train_epochs: 1

# optimizer
adam_beta1: 0.9
adam_beta2: 0.95
weight_decay: 0.1
warmup_steps: 100
learning_rate: 5.3e-4

# vllm settings
use_vllm: true
trl_vllm_server_port: 8880

# logging
logging_steps: 10
logging_first_step: true
log_completions: true

# device allocation
trl_vllm_server_device: cuda:0
sentiment_reward_device: cuda:1

# model path
model_path: Qwen/Qwen3-4B
base_agent_path: Qwen/Qwen3-8B
