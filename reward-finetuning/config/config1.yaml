learning_rate: 2.0e-5
num_train_epochs: 5.0
per_device_train_batch_size: 8
gradient_accumulation_steps: 16

lora_rank: 64
lora_target: all
lora_alpha: 128